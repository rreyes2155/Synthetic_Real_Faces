{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "LlkfHI0qMe9a"
      ],
      "toc_visible": true,
      "mount_file_id": "111OdFM4_Z_FACD38wdnRWqiLDR2Nqk0M",
      "authorship_tag": "ABX9TyNIfuG30xiJ3kxvYRs91kBj",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rreyes2155/Synthetic_Real_Faces/blob/main/DATA_690_Deep_Learning_Project_Models_GCP.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ['USE_AUTH_EPHEM'] = '0'\n",
        "\n",
        "from google.colab import auth\n",
        "auth.authenticate_user()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ihRuwvU19brg",
        "outputId": "933a37df-ec79-4e1d-943c-f1b424ff3f37"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:google.auth.compute_engine._metadata:Compute Engine Metadata server unavailable on attempt 1 of 3. Reason: [Errno 115] Operation now in progress\n",
            "WARNING:google.auth.compute_engine._metadata:Compute Engine Metadata server unavailable on attempt 2 of 3. Reason: [Errno 115] Operation now in progress\n",
            "WARNING:google.auth.compute_engine._metadata:Compute Engine Metadata server unavailable on attempt 3 of 3. Reason: [Errno 115] Operation now in progress\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# install gcsfuse so we can load our bucket as local file\n",
        "!echo \"deb http://packages.cloud.google.com/apt gcsfuse-bionic main\" > /etc/apt/sources.list.d/gcsfuse.list\n",
        "!curl https://packages.cloud.google.com/apt/doc/apt-key.gpg | apt-key add -\n",
        "!apt -qq update\n",
        "!apt -qq install gcsfuse"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qpVd5hy39fMD",
        "outputId": "78e39cfe-d625-4769-f559-010d1aa30326"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "\r  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\r100  1210  100  1210    0     0  43214      0 --:--:-- --:--:-- --:--:-- 43214\n",
            "OK\n",
            "72 packages can be upgraded. Run 'apt list --upgradable' to see them.\n",
            "gcsfuse is already the newest version (0.42.4).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 72 not upgraded.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# make directory and load our bucket to directory\n",
        "!mkdir faces\n",
        "!gcsfuse --implicit-dirs data690deeplearning faces"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4aQk4mrT9j_b",
        "outputId": "bb85a6ae-b68a-4f11-c6ad-4d002d91443e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "mkdir: cannot create directory ‘faces’: File exists\n",
            "I0514 05:36:53.414283 2023/05/14 05:36:53.414256 Start gcsfuse/0.42.4 (Go version go1.19.7) for app \"\" using mount point: /content/faces\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9BWgKJWgw9fS"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "import os\n",
        "from skimage import io\n",
        "import random\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import transforms, datasets, models\n",
        "from torchvision.transforms import ToTensor\n",
        "from torchvision.io import read_image\n",
        "from PIL import Image\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchvision\n",
        "import time\n",
        "import copy\n",
        "from transformers import ViTImageProcessor, ViTForImageClassification, TrainingArguments, Trainer, ViTFeatureExtractor, ViTImageProcessor\n",
        "from sklearn.metrics import accuracy_score\n",
        "from datasets import Dataset, load_metric"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "device"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "tIh9coV0LuL1",
        "outputId": "73af0bd9-fa8e-47fc-d586-df2b616be529"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'cuda'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Load Cleaned DataFrames and create train/test splits"
      ],
      "metadata": {
        "id": "tnzEjd2vclo5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# upload cleaned metadata csvs for each dataset\n",
        "real = pd.read_csv('faces/celebahq_metadata_cleaned.csv')\n",
        "lama_all = pd.read_csv('faces/lama_all_metadata_cleaned.csv')\n",
        "mat = pd.read_csv('faces/mat_metadata_cleaned.csv')\n",
        "\n",
        "# We may not use them but lets load the seperated lama datasets in case we want to try.\n",
        "lama_deep = pd.read_csv('faces/lama_deep_metadata_cleaned.csv')\n",
        "lama_dialated = pd.read_csv('faces/lama_dialated_metadata_cleaned.csv')\n",
        "lama_fourier = pd.read_csv('faces/lama_fourier_metadata_cleaned.csv')"
      ],
      "metadata": {
        "id": "_pcWEh0zKHrX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Combine data and create train/testsplit"
      ],
      "metadata": {
        "id": "T7Xl4zy4VslQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# function to combine datasets and create train/test\n",
        "\n",
        "# accepts a list of the number of samples you want from each dataset\n",
        "# in this order [real, lama_all, 'mat', 'deep', 'dialated', 'fourier']\n",
        "\n",
        "def make_test_split(nums, test_portion = 0.2, rs = 3):\n",
        "    # check we don't pull from lama_all and any of the lama types datasets\n",
        "    if nums[1] > 0 and (nums[3] > 0 or nums[4] > 0 or nums[5] > 0):\n",
        "        return null\n",
        "\n",
        "    # sample each dataset\n",
        "    a = real.sample(n = nums[0], random_state = rs, ignore_index = True)\n",
        "    if nums[1] != 0:\n",
        "        b = lama_all.sample(n = nums[1], random_state = rs, ignore_index = True)\n",
        "    c = mat.sample(n = nums[2], random_state = rs, ignore_index = True)\n",
        "    if nums[1] == 0:\n",
        "        d = lama_deep.sample(n = nums[3], random_state = rs, ignore_index = True)\n",
        "        e = lama_dialated.sample(n = nums[4], random_state = rs, ignore_index = True)\n",
        "        f = lama_fourier.sample(n = nums[5], random_state = rs, ignore_index = True)\n",
        "\n",
        "    # create train/test splits for each dataset\n",
        "    train_a, test_a = train_test_split(a, test_size = test_portion, random_state = rs)\n",
        "    if nums[1] != 0:\n",
        "        train_b, test_b = train_test_split(b, test_size = test_portion, random_state = rs)\n",
        "    train_c, test_c = train_test_split(c, test_size = test_portion, random_state = rs)\n",
        "    if nums[1] == 0:\n",
        "        train_d, test_d = train_test_split(d, test_size = test_portion, random_state = rs)\n",
        "        train_e, test_e = train_test_split(e, test_size = test_portion, random_state = rs)\n",
        "        train_f, test_f = train_test_split(f, test_size = test_portion, random_state = rs)\n",
        "\n",
        "    # list of frames to concat\n",
        "    if nums[1] != 0:        \n",
        "        train_frames = [train_a, train_b, train_c]\n",
        "        test_frames = [test_a, test_b, test_c]\n",
        "    if nums[1] == 0:        \n",
        "        train_frames = [train_a, train_c, train_d, train_e, train_f]\n",
        "        test_frames = [test_a, test_c, test_d, test_e, test_f]\n",
        "\n",
        "    # concat train/test dfs\n",
        "    train = pd.concat(train_frames, ignore_index = True)\n",
        "    test = pd.concat(test_frames, ignore_index = True)\n",
        "\n",
        "    # shuffle data\n",
        "    train_shuff = train.sample(frac=1).reset_index(drop=True)\n",
        "    test_shuff = test.sample(frac=1).reset_index(drop=True)\n",
        "    \n",
        "    return train_shuff, test_shuff"
      ],
      "metadata": {
        "id": "RqLfk_N-SY0Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# lets get counts to see how big we can make our datasets\n",
        "data_list = [real, lama_all, mat, lama_deep, lama_dialated, lama_fourier]\n",
        "data_names = ['real', 'lama_all', 'mat', 'lama_deep', 'lama_dialated', 'lama_fourier']\n",
        "counts = []\n",
        "for df in data_list:\n",
        "    counts.append(len(df))\n",
        "    counts_df = pd.DataFrame(list(zip(data_names, counts)), columns = ['DataFrame', 'Number of Samples'])\n",
        "display(counts_df)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 235
        },
        "id": "R5_jVeFcuHQ0",
        "outputId": "6eb453cf-56df-46f9-e60d-b2fc9028dc87"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "       DataFrame  Number of Samples\n",
              "0           real               9276\n",
              "1       lama_all               8784\n",
              "2            mat               7637\n",
              "3      lama_deep               3483\n",
              "4  lama_dialated               3483\n",
              "5   lama_fourier               1818"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-05467194-b09d-4bf7-b110-aef8eb0185d9\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>DataFrame</th>\n",
              "      <th>Number of Samples</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>real</td>\n",
              "      <td>9276</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>lama_all</td>\n",
              "      <td>8784</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>mat</td>\n",
              "      <td>7637</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>lama_deep</td>\n",
              "      <td>3483</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>lama_dialated</td>\n",
              "      <td>3483</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>lama_fourier</td>\n",
              "      <td>1818</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-05467194-b09d-4bf7-b110-aef8eb0185d9')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-05467194-b09d-4bf7-b110-aef8eb0185d9 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-05467194-b09d-4bf7-b110-aef8eb0185d9');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# create train/test data\n",
        "\n",
        "# [real, lama_all, 'mat', 'deep', 'dialated', 'fourier']\n",
        "# lest first start with a small sample \n",
        "small_sample_sizes = [500, 250, 250, 0, 0, 0]\n",
        "all_samples = [len(real), len(lama_all), len(mat), 0, 0, 0]\n",
        "\n",
        "train_1000, test_1000 = make_test_split(small_sample_sizes)\n",
        "train_all, test_all = make_test_split(small_sample_sizes)"
      ],
      "metadata": {
        "id": "SUuOmBLEcDyw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Models"
      ],
      "metadata": {
        "id": "WdhBCF_ZgE45"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Create Custom Dataset"
      ],
      "metadata": {
        "id": "XXbUAGh-3O4Q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We need to first create a custom dataset for our data. Lets first look at the shape of the images"
      ],
      "metadata": {
        "id": "BXjKwCxQiewR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "img_shapes = {}\n",
        "for i in range(100):      \n",
        "    shape = read_image(train_1000['image_path'].iloc[random.randint(0, len(train_1000)) - 1]).size()\n",
        "    img_shapes[shape] = img_shapes.get(shape, 0) + 1"
      ],
      "metadata": {
        "id": "1a0ISS_smxmO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for key in img_shapes:\n",
        "    print(f'{key} : {img_shapes[key]}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sQRsk5-aoPB0",
        "outputId": "6f7456e3-551f-443c-a07e-0f1a4234d747"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([3, 200, 200]) : 100\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "It looks like our data is the same shape, we will adjust it later when using pretrained models."
      ],
      "metadata": {
        "id": "M-k2GUhIwu7v"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now lets make a custom dataset\n",
        "\n",
        "https://pytorch.org/tutorials/beginner/basics/data_tutorial.html"
      ],
      "metadata": {
        "id": "nQ3ukmEOxsPV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# create custom dataset class for our faces  dataset\n",
        "\n",
        "class Faces(Dataset):\n",
        "    def __init__(self, df, transform):\n",
        "        self.df = df\n",
        "        self.transform = transform\n",
        "        self.classes = ['real', 'fake']\n",
        "        self.class_to_idx = dict_2 = {'real' : 0, 'fake': 1}\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.df)\n",
        "\n",
        "    # retrives image from path in df file and target from df\n",
        "    def get_image_class(self, index):\n",
        "        image_path = self.df.iloc[index]['image_path']\n",
        "        class_name = self.df.iloc[index]['target']\n",
        "        return Image.open(image_path), class_name \n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        image, class_name = self.get_image_class(idx)\n",
        "        label = self.class_to_idx[class_name]\n",
        "\n",
        "        return self.transform(image), label"
      ],
      "metadata": {
        "id": "MJUHPMIsidQn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training and Helper Functions"
      ],
      "metadata": {
        "id": "LlkfHI0qMe9a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# from https://pytorch.org/tutorials/beginner/finetuning_torchvision_models_tutorial.\n",
        "# code to train model\n",
        "\n",
        "def train_model(model, dataloaders, criterion, optimizer, num_epochs=25, is_inception=False):\n",
        "    since = time.time()\n",
        "\n",
        "    val_acc_history = []\n",
        "\n",
        "    best_model_wts = copy.deepcopy(model.state_dict())\n",
        "    best_acc = 0.0\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        print('Epoch {}/{}'.format(epoch, num_epochs - 1))\n",
        "        print('-' * 10)\n",
        "\n",
        "        # Each epoch has a training and validation phase\n",
        "        for phase in ['train', 'val']:\n",
        "            if phase == 'train':\n",
        "                model.train()  # Set model to training mode\n",
        "            else:\n",
        "                model.eval()   # Set model to evaluate mode\n",
        "\n",
        "            running_loss = 0.0\n",
        "            running_corrects = 0\n",
        "\n",
        "            # Iterate over data.\n",
        "            for inputs, labels in dataloaders[phase]:\n",
        "                inputs = inputs.to(device)\n",
        "                labels = labels.to(device)\n",
        "\n",
        "                # zero the parameter gradients\n",
        "                optimizer.zero_grad()\n",
        "\n",
        "                # forward\n",
        "                # track history if only in train\n",
        "                with torch.set_grad_enabled(phase == 'train'):\n",
        "                    # Get model outputs and calculate loss\n",
        "                    # Special case for inception because in training it has an auxiliary output. In train\n",
        "                    #   mode we calculate the loss by summing the final output and the auxiliary output\n",
        "                    #   but in testing we only consider the final output.\n",
        "                    if is_inception and phase == 'train':\n",
        "                        # From https://discuss.pytorch.org/t/how-to-optimize-inception-model-with-auxiliary-classifiers/7958\n",
        "                        outputs, aux_outputs = model(inputs)\n",
        "                        loss1 = criterion(outputs, labels)\n",
        "                        loss2 = criterion(aux_outputs, labels)\n",
        "                        loss = loss1 + 0.4*loss2\n",
        "                    else:\n",
        "                        outputs = model(inputs)\n",
        "                        loss = criterion(outputs, labels)\n",
        "\n",
        "                    _, preds = torch.max(outputs, 1)\n",
        "\n",
        "                    # backward + optimize only if in training phase\n",
        "                    if phase == 'train':\n",
        "                        loss.backward()\n",
        "                        optimizer.step()\n",
        "\n",
        "                # statistics\n",
        "                running_loss += loss.item() * inputs.size(0)\n",
        "                running_corrects += torch.sum(preds == labels.data)\n",
        "\n",
        "            epoch_loss = running_loss / len(dataloaders[phase].dataset)\n",
        "            epoch_acc = running_corrects.double() / len(dataloaders[phase].dataset)\n",
        "\n",
        "            print('{} Loss: {:.4f} Acc: {:.4f}'.format(phase, epoch_loss, epoch_acc))\n",
        "\n",
        "            # deep copy the model\n",
        "            if phase == 'val' and epoch_acc > best_acc:\n",
        "                best_acc = epoch_acc\n",
        "                best_model_wts = copy.deepcopy(model.state_dict())\n",
        "            if phase == 'val':\n",
        "                val_acc_history.append(epoch_acc)\n",
        "\n",
        "        print()\n",
        "\n",
        "    time_elapsed = time.time() - since\n",
        "    print('Training complete in {:.0f}m {:.0f}s'.format(time_elapsed // 60, time_elapsed % 60))\n",
        "    print('Best val Acc: {:4f}'.format(best_acc))\n",
        "\n",
        "    # load best model weights\n",
        "    model.load_state_dict(best_model_wts)\n",
        "    return model, val_acc_history"
      ],
      "metadata": {
        "id": "0F5wZoUbJjkm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Densnet Model"
      ],
      "metadata": {
        "id": "bsFQa9RIqhLQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://pytorch.org/tutorials/beginner/finetuning_torchvision_models_tutorial.html\n",
        "\n",
        "\n",
        "https://pytorch.org/vision/main/models.html\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "Hho2Cuz5zsQ1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "All pre-trained models expect input images normalized in the same way, i.e. mini-batches of 3-channel RGB images of shape (3 x H x W), where H and W are expected to be at least 224. The images have to be loaded in to a range of [0, 1] and then normalized using mean = [0.485, 0.456, 0.406] and std = [0.229, 0.224, 0.225]."
      ],
      "metadata": {
        "id": "7gylWKDzN0Aq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# transformations for densenet model\n",
        "densenet_transforms = transforms.Compose([\n",
        "    transforms.Resize(224),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "])     "
      ],
      "metadata": {
        "id": "gdff8EPEQStF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# set batch size\n",
        "batch_size = 32"
      ],
      "metadata": {
        "id": "nSGFhtpXRx32"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# create Dataset for train and test\n",
        "train_1000_densenet_dataset = Faces(train_1000, transform = densenet_transforms)\n",
        "train_all_densenet_dataset = Faces(train_all, transform = densenet_transforms)\n",
        "\n",
        "test_1000_densenet_dataset = Faces(test_1000, transform = densenet_transforms)\n",
        "test_all_densenet_dataset = Faces(test_all, transform = densenet_transforms)\n",
        "\n",
        "# create Dataloader for train and test\n",
        "train_1000_densenet_dataloader = DataLoader(train_1000_densenet_dataset, batch_size = batch_size, shuffle = True)\n",
        "train_all_densenet_dataloader = DataLoader(train_all_densenet_dataset, batch_size = batch_size, shuffle = True)\n",
        "\n",
        "test_1000_densenet_dataloader = DataLoader(test_1000_densenet_dataset, batch_size = batch_size, shuffle = True)\n",
        "test_all_densenet_dataloader = DataLoader(test_all_densenet_dataset, batch_size = batch_size, shuffle = True)"
      ],
      "metadata": {
        "id": "_fD1zvhgQFTV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# create test/train dataloader dicts\n",
        "densenet_1000_dict = {'train' : train_1000_densenet_dataloader, 'val' : test_1000_densenet_dataloader}\n",
        "densenet_all_dict = {'train' : train_all_densenet_dataloader, 'val' : test_all_densenet_dataloader}"
      ],
      "metadata": {
        "id": "Fc7DMSFlUxjk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Subset Run"
      ],
      "metadata": {
        "id": "ELebKc3xs--4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# create model\n",
        "densenet_model = models.densenet121(pretrained=True)\n",
        "in_features = densenet_model.classifier.in_features\n",
        "densenet_model.classifier = nn.Linear(in_features, 2)\n",
        "input_size = 224\n",
        "densenet_model.to(device)"
      ],
      "metadata": {
        "id": "jOjXxVSrykIt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "loss_fn = torch.nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.SGD(densenet_model.parameters(), lr=0.001, momentum=0.9)"
      ],
      "metadata": {
        "id": "aGR10RYmIiCF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "epochs = 5"
      ],
      "metadata": {
        "id": "u79o9o6NUMts"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "densenet_model_tuned_1000, densetnet_1000_results = train_model(densenet_model, densenet_1000_dict,\n",
        "                                                                loss_fn, optimizer, num_epochs = epochs)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kQFEJ80wNp5z",
        "outputId": "de0b939e-6c3a-4b0a-ef76-9c1add86757a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0/4\n",
            "----------\n",
            "train Loss: 0.6920 Acc: 0.5587\n",
            "val Loss: 0.6250 Acc: 0.6450\n",
            "\n",
            "Epoch 1/4\n",
            "----------\n",
            "train Loss: 0.4556 Acc: 0.8425\n",
            "val Loss: 0.4769 Acc: 0.8100\n",
            "\n",
            "Epoch 2/4\n",
            "----------\n",
            "train Loss: 0.2777 Acc: 0.9325\n",
            "val Loss: 0.4112 Acc: 0.8050\n",
            "\n",
            "Epoch 3/4\n",
            "----------\n",
            "train Loss: 0.1646 Acc: 0.9637\n",
            "val Loss: 0.3869 Acc: 0.8300\n",
            "\n",
            "Epoch 4/4\n",
            "----------\n",
            "train Loss: 0.0964 Acc: 0.9888\n",
            "val Loss: 0.3867 Acc: 0.8250\n",
            "\n",
            "Training complete in 1m 23s\n",
            "Best val Acc: 0.830000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### All Data Run with SGD"
      ],
      "metadata": {
        "id": "xJJoWfwmtMsA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# re instantiate model\n",
        "densenet_model = models.densenet121(pretrained=True)\n",
        "in_features = densenet_model.classifier.in_features\n",
        "densenet_model.classifier = nn.Linear(in_features, 2)\n",
        "densenet_model.to(device)\n",
        "\n",
        "loss_fn = torch.nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.SGD(densenet_model.parameters(), lr=0.001, momentum=0.9)"
      ],
      "metadata": {
        "id": "k4z3QCQwnbCh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "epochs = 25"
      ],
      "metadata": {
        "id": "iMQe5TDwZ0Dw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "densenet_model_tuned_all, densetnet_all_results = train_model(densenet_model, densenet_all_dict,\n",
        "                                                                loss_fn, optimizer, num_epochs = epochs)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FuTN4oW9Sje3",
        "outputId": "6e542171-9aae-4884-8072-3fa834020ea7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0/24\n",
            "----------\n",
            "train Loss: 0.7487 Acc: 0.5038\n",
            "val Loss: 0.6110 Acc: 0.7000\n",
            "\n",
            "Epoch 1/24\n",
            "----------\n",
            "train Loss: 0.4766 Acc: 0.8363\n",
            "val Loss: 0.4999 Acc: 0.7400\n",
            "\n",
            "Epoch 2/24\n",
            "----------\n",
            "train Loss: 0.3093 Acc: 0.9075\n",
            "val Loss: 0.4467 Acc: 0.7900\n",
            "\n",
            "Epoch 3/24\n",
            "----------\n",
            "train Loss: 0.2220 Acc: 0.9350\n",
            "val Loss: 0.4210 Acc: 0.7900\n",
            "\n",
            "Epoch 4/24\n",
            "----------\n",
            "train Loss: 0.1260 Acc: 0.9813\n",
            "val Loss: 0.4085 Acc: 0.8000\n",
            "\n",
            "Epoch 5/24\n",
            "----------\n",
            "train Loss: 0.0654 Acc: 0.9938\n",
            "val Loss: 0.4372 Acc: 0.8100\n",
            "\n",
            "Epoch 6/24\n",
            "----------\n",
            "train Loss: 0.0534 Acc: 0.9962\n",
            "val Loss: 0.4355 Acc: 0.8250\n",
            "\n",
            "Epoch 7/24\n",
            "----------\n",
            "train Loss: 0.0267 Acc: 1.0000\n",
            "val Loss: 0.4554 Acc: 0.8100\n",
            "\n",
            "Epoch 8/24\n",
            "----------\n",
            "train Loss: 0.0251 Acc: 0.9975\n",
            "val Loss: 0.4497 Acc: 0.8250\n",
            "\n",
            "Epoch 9/24\n",
            "----------\n",
            "train Loss: 0.0136 Acc: 1.0000\n",
            "val Loss: 0.4630 Acc: 0.8200\n",
            "\n",
            "Epoch 10/24\n",
            "----------\n",
            "train Loss: 0.0139 Acc: 1.0000\n",
            "val Loss: 0.4782 Acc: 0.8100\n",
            "\n",
            "Epoch 11/24\n",
            "----------\n",
            "train Loss: 0.0155 Acc: 0.9975\n",
            "val Loss: 0.4882 Acc: 0.8100\n",
            "\n",
            "Epoch 12/24\n",
            "----------\n",
            "train Loss: 0.0078 Acc: 1.0000\n",
            "val Loss: 0.4969 Acc: 0.7950\n",
            "\n",
            "Epoch 13/24\n",
            "----------\n",
            "train Loss: 0.0084 Acc: 1.0000\n",
            "val Loss: 0.4995 Acc: 0.8150\n",
            "\n",
            "Epoch 14/24\n",
            "----------\n",
            "train Loss: 0.0072 Acc: 1.0000\n",
            "val Loss: 0.5081 Acc: 0.8250\n",
            "\n",
            "Epoch 15/24\n",
            "----------\n",
            "train Loss: 0.0063 Acc: 1.0000\n",
            "val Loss: 0.5162 Acc: 0.8250\n",
            "\n",
            "Epoch 16/24\n",
            "----------\n",
            "train Loss: 0.0053 Acc: 1.0000\n",
            "val Loss: 0.5207 Acc: 0.8150\n",
            "\n",
            "Epoch 17/24\n",
            "----------\n",
            "train Loss: 0.0050 Acc: 1.0000\n",
            "val Loss: 0.5193 Acc: 0.8150\n",
            "\n",
            "Epoch 18/24\n",
            "----------\n",
            "train Loss: 0.0045 Acc: 1.0000\n",
            "val Loss: 0.5152 Acc: 0.8200\n",
            "\n",
            "Epoch 19/24\n",
            "----------\n",
            "train Loss: 0.0053 Acc: 1.0000\n",
            "val Loss: 0.5198 Acc: 0.8200\n",
            "\n",
            "Epoch 20/24\n",
            "----------\n",
            "train Loss: 0.0029 Acc: 1.0000\n",
            "val Loss: 0.5334 Acc: 0.8300\n",
            "\n",
            "Epoch 21/24\n",
            "----------\n",
            "train Loss: 0.0058 Acc: 1.0000\n",
            "val Loss: 0.5296 Acc: 0.8300\n",
            "\n",
            "Epoch 22/24\n",
            "----------\n",
            "train Loss: 0.0051 Acc: 1.0000\n",
            "val Loss: 0.5371 Acc: 0.8250\n",
            "\n",
            "Epoch 23/24\n",
            "----------\n",
            "train Loss: 0.0054 Acc: 1.0000\n",
            "val Loss: 0.5744 Acc: 0.8150\n",
            "\n",
            "Epoch 24/24\n",
            "----------\n",
            "train Loss: 0.0062 Acc: 0.9988\n",
            "val Loss: 0.5690 Acc: 0.8300\n",
            "\n",
            "Training complete in 6m 33s\n",
            "Best val Acc: 0.830000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### All Data Run with Adam"
      ],
      "metadata": {
        "id": "isI7OjHatdlL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# re instantiate model\n",
        "densenet_model = models.densenet121(pretrained=True)\n",
        "in_features = densenet_model.classifier.in_features\n",
        "densenet_model.classifier = nn.Linear(in_features, 2)\n",
        "densenet_model.to(device)\n",
        "\n",
        "loss_fn = torch.nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.AdamW(densenet_model.parameters(), lr=0.001)\n",
        "\n",
        "epochs = 25"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Km-NiEixcF3u",
        "outputId": "83c1a7a9-39bf-472e-8bfe-0d53d105a9de"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=DenseNet121_Weights.IMAGENET1K_V1`. You can also use `weights=DenseNet121_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "densenet_model_adam, densetnet_adam_results = train_model(densenet_model, densenet_all_dict,\n",
        "                                                                loss_fn, optimizer, num_epochs = epochs)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kZtZMJ-quX1V",
        "outputId": "a85e8e10-0944-420f-a26c-51d055954e7d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0/24\n",
            "----------\n",
            "train Loss: 0.6151 Acc: 0.6900\n",
            "val Loss: 2.4410 Acc: 0.5650\n",
            "\n",
            "Epoch 1/24\n",
            "----------\n",
            "train Loss: 0.3802 Acc: 0.8300\n",
            "val Loss: 0.5430 Acc: 0.7750\n",
            "\n",
            "Epoch 2/24\n",
            "----------\n",
            "train Loss: 0.2797 Acc: 0.8950\n",
            "val Loss: 0.5493 Acc: 0.8300\n",
            "\n",
            "Epoch 3/24\n",
            "----------\n",
            "train Loss: 0.2686 Acc: 0.8788\n",
            "val Loss: 0.4644 Acc: 0.8150\n",
            "\n",
            "Epoch 4/24\n",
            "----------\n",
            "train Loss: 0.2625 Acc: 0.8888\n",
            "val Loss: 0.7016 Acc: 0.7650\n",
            "\n",
            "Epoch 5/24\n",
            "----------\n",
            "train Loss: 0.2430 Acc: 0.8962\n",
            "val Loss: 0.9307 Acc: 0.7350\n",
            "\n",
            "Epoch 6/24\n",
            "----------\n",
            "train Loss: 0.1810 Acc: 0.9287\n",
            "val Loss: 4.5883 Acc: 0.5700\n",
            "\n",
            "Epoch 7/24\n",
            "----------\n",
            "train Loss: 0.1733 Acc: 0.9325\n",
            "val Loss: 0.6587 Acc: 0.7650\n",
            "\n",
            "Epoch 8/24\n",
            "----------\n",
            "train Loss: 0.0876 Acc: 0.9738\n",
            "val Loss: 1.3248 Acc: 0.7450\n",
            "\n",
            "Epoch 9/24\n",
            "----------\n",
            "train Loss: 0.0564 Acc: 0.9800\n",
            "val Loss: 1.4449 Acc: 0.7300\n",
            "\n",
            "Epoch 10/24\n",
            "----------\n",
            "train Loss: 0.0739 Acc: 0.9750\n",
            "val Loss: 1.9254 Acc: 0.6450\n",
            "\n",
            "Epoch 11/24\n",
            "----------\n",
            "train Loss: 0.1922 Acc: 0.9225\n",
            "val Loss: 0.8826 Acc: 0.8100\n",
            "\n",
            "Epoch 12/24\n",
            "----------\n",
            "train Loss: 0.1557 Acc: 0.9450\n",
            "val Loss: 2.1079 Acc: 0.6400\n",
            "\n",
            "Epoch 13/24\n",
            "----------\n",
            "train Loss: 0.1348 Acc: 0.9375\n",
            "val Loss: 0.5429 Acc: 0.8400\n",
            "\n",
            "Epoch 14/24\n",
            "----------\n",
            "train Loss: 0.0717 Acc: 0.9725\n",
            "val Loss: 2.5730 Acc: 0.6150\n",
            "\n",
            "Epoch 15/24\n",
            "----------\n",
            "train Loss: 0.0409 Acc: 0.9838\n",
            "val Loss: 0.8605 Acc: 0.8150\n",
            "\n",
            "Epoch 16/24\n",
            "----------\n",
            "train Loss: 0.0601 Acc: 0.9800\n",
            "val Loss: 0.8219 Acc: 0.7750\n",
            "\n",
            "Epoch 17/24\n",
            "----------\n",
            "train Loss: 0.0526 Acc: 0.9825\n",
            "val Loss: 0.6494 Acc: 0.7750\n",
            "\n",
            "Epoch 18/24\n",
            "----------\n",
            "train Loss: 0.0527 Acc: 0.9800\n",
            "val Loss: 1.7397 Acc: 0.7400\n",
            "\n",
            "Epoch 19/24\n",
            "----------\n",
            "train Loss: 0.0484 Acc: 0.9800\n",
            "val Loss: 0.7297 Acc: 0.8550\n",
            "\n",
            "Epoch 20/24\n",
            "----------\n",
            "train Loss: 0.0570 Acc: 0.9775\n",
            "val Loss: 0.8752 Acc: 0.8050\n",
            "\n",
            "Epoch 21/24\n",
            "----------\n",
            "train Loss: 0.0946 Acc: 0.9625\n",
            "val Loss: 1.0201 Acc: 0.7600\n",
            "\n",
            "Epoch 22/24\n",
            "----------\n",
            "train Loss: 0.0765 Acc: 0.9725\n",
            "val Loss: 1.9632 Acc: 0.6450\n",
            "\n",
            "Epoch 23/24\n",
            "----------\n",
            "train Loss: 0.0734 Acc: 0.9738\n",
            "val Loss: 1.3208 Acc: 0.7300\n",
            "\n",
            "Epoch 24/24\n",
            "----------\n",
            "train Loss: 0.0669 Acc: 0.9713\n",
            "val Loss: 0.6467 Acc: 0.8250\n",
            "\n",
            "Training complete in 6m 31s\n",
            "Best val Acc: 0.855000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Resnet Model"
      ],
      "metadata": {
        "id": "ew4u5OR3xxac"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The images are resized to resize_size=[256] using interpolation=InterpolationMode.BILINEAR, followed by a central crop of crop_size=[224]. Finally the values are first rescaled to [0.0, 1.0] and then normalized using mean=[0.485, 0.456, 0.406] and std=[0.229, 0.224, 0.225]."
      ],
      "metadata": {
        "id": "clignTs7yhXJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# transformations for resnet model\n",
        "resnet_transforms = transforms.Compose([\n",
        "    transforms.Resize(256),\n",
        "    transforms.CenterCrop(224),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "])     \n",
        "\n",
        "# create Dataset for train and test\n",
        "train_1000_resnet_dataset = Faces(train_1000, transform = resnet_transforms)\n",
        "train_all_resnet_dataset = Faces(train_all, transform = resnet_transforms)\n",
        "\n",
        "test_1000_resnet_dataset = Faces(test_1000, transform = resnet_transforms)\n",
        "test_all_resnet_dataset = Faces(test_all, transform = resnet_transforms)\n",
        "\n",
        "# create Dataloader for train and test\n",
        "train_1000_resnet_dataloader = DataLoader(train_1000_resnet_dataset, batch_size = batch_size, shuffle = True)\n",
        "train_all_resnet_dataloader = DataLoader(train_all_resnet_dataset, batch_size = batch_size, shuffle = True)\n",
        "\n",
        "test_1000_resnet_dataloader = DataLoader(test_1000_resnet_dataset, batch_size = batch_size, shuffle = True)\n",
        "test_all_resnet_dataloader = DataLoader(test_all_resnet_dataset, batch_size = batch_size, shuffle = True)\n",
        "\n",
        "# create test/train dataloader dicts\n",
        "resnet_1000_dict = {'train' : train_1000_resnet_dataloader, 'val' : test_1000_resnet_dataloader}\n",
        "resnet_all_dict = {'train' : train_all_resnet_dataloader, 'val' : test_all_resnet_dataloader}"
      ],
      "metadata": {
        "id": "BpDWQAZZzbhm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Subset Run\n"
      ],
      "metadata": {
        "id": "T5A8TEfJ03er"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "resnet_model = models.resnet18(pretrained=True)\n",
        "in_features = resnet_model.fc.in_features\n",
        "resnet_model.classifier = nn.Linear(in_features, 2)\n",
        "resnet_model.to(device)\n",
        "\n",
        "loss_fn = torch.nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.SGD(resnet_model.parameters(), lr=0.001, momentum=0.9)\n",
        "#optimizer = torch.optim.AdamW(resnet_model.parameters(), lr=0.001)\n",
        "\n",
        "epochs = 5"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ATTacUSQyBrA",
        "outputId": "7c2c763f-5afb-4532-ad9f-a38dfdd6caf6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "resnet_model_1000, ressetnet_1000_results = train_model(resnet_model, resnet_all_dict,\n",
        "                                                                loss_fn, optimizer, num_epochs = epochs)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0tVXrG-A1cFe",
        "outputId": "1b4f46e3-0374-4da7-a853-8f416f6a49e4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0/4\n",
            "----------\n",
            "train Loss: 3.1331 Acc: 0.4188\n",
            "val Loss: 0.7500 Acc: 0.6250\n",
            "\n",
            "Epoch 1/4\n",
            "----------\n",
            "train Loss: 0.2457 Acc: 0.9000\n",
            "val Loss: 0.5471 Acc: 0.7300\n",
            "\n",
            "Epoch 2/4\n",
            "----------\n",
            "train Loss: 0.0834 Acc: 0.9825\n",
            "val Loss: 0.5127 Acc: 0.7900\n",
            "\n",
            "Epoch 3/4\n",
            "----------\n",
            "train Loss: 0.0447 Acc: 0.9988\n",
            "val Loss: 0.4855 Acc: 0.8000\n",
            "\n",
            "Epoch 4/4\n",
            "----------\n",
            "train Loss: 0.0199 Acc: 1.0000\n",
            "val Loss: 0.4875 Acc: 0.8100\n",
            "\n",
            "Training complete in 0m 52s\n",
            "Best val Acc: 0.810000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### All Data Run with SGD"
      ],
      "metadata": {
        "id": "de3r5JCf4BRP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "resnet_model = models.resnet18(pretrained=True)\n",
        "in_features = resnet_model.fc.in_features\n",
        "resnet_model.classifier = nn.Linear(in_features, 2)\n",
        "resnet_model.to(device)\n",
        "\n",
        "loss_fn = torch.nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.SGD(resnet_model.parameters(), lr=0.001, momentum=0.9)\n",
        "#optimizer = torch.optim.AdamW(resnet_model.parameters(), lr=0.001)\n",
        "\n",
        "epochs = 25"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2YWg37mZ4M4b",
        "outputId": "dbee3794-840d-457e-e765-24a72e70b32d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "resnet_model_all_sgd, ressetnet_all_sgd_results = train_model(resnet_model, resnet_all_dict,\n",
        "                                                                loss_fn, optimizer, num_epochs = epochs)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JUG1X7AK4fta",
        "outputId": "9acf6e81-5955-4169-a57f-50bcdb36aa5b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0/24\n",
            "----------\n",
            "train Loss: 3.1167 Acc: 0.4163\n",
            "val Loss: 0.8946 Acc: 0.6150\n",
            "\n",
            "Epoch 1/24\n",
            "----------\n",
            "train Loss: 0.2703 Acc: 0.8938\n",
            "val Loss: 0.5745 Acc: 0.7400\n",
            "\n",
            "Epoch 2/24\n",
            "----------\n",
            "train Loss: 0.0943 Acc: 0.9800\n",
            "val Loss: 0.5301 Acc: 0.7600\n",
            "\n",
            "Epoch 3/24\n",
            "----------\n",
            "train Loss: 0.0387 Acc: 0.9988\n",
            "val Loss: 0.5340 Acc: 0.7900\n",
            "\n",
            "Epoch 4/24\n",
            "----------\n",
            "train Loss: 0.0211 Acc: 1.0000\n",
            "val Loss: 0.5054 Acc: 0.8150\n",
            "\n",
            "Epoch 5/24\n",
            "----------\n",
            "train Loss: 0.0168 Acc: 1.0000\n",
            "val Loss: 0.5083 Acc: 0.8250\n",
            "\n",
            "Epoch 6/24\n",
            "----------\n",
            "train Loss: 0.0109 Acc: 1.0000\n",
            "val Loss: 0.5183 Acc: 0.8000\n",
            "\n",
            "Epoch 7/24\n",
            "----------\n",
            "train Loss: 0.0078 Acc: 1.0000\n",
            "val Loss: 0.5205 Acc: 0.8100\n",
            "\n",
            "Epoch 8/24\n",
            "----------\n",
            "train Loss: 0.0112 Acc: 1.0000\n",
            "val Loss: 0.5193 Acc: 0.8200\n",
            "\n",
            "Epoch 9/24\n",
            "----------\n",
            "train Loss: 0.0078 Acc: 1.0000\n",
            "val Loss: 0.5184 Acc: 0.8150\n",
            "\n",
            "Epoch 10/24\n",
            "----------\n",
            "train Loss: 0.0064 Acc: 1.0000\n",
            "val Loss: 0.5132 Acc: 0.8350\n",
            "\n",
            "Epoch 11/24\n",
            "----------\n",
            "train Loss: 0.0153 Acc: 1.0000\n",
            "val Loss: 0.5530 Acc: 0.8050\n",
            "\n",
            "Epoch 12/24\n",
            "----------\n",
            "train Loss: 0.0097 Acc: 1.0000\n",
            "val Loss: 0.5432 Acc: 0.8200\n",
            "\n",
            "Epoch 13/24\n",
            "----------\n",
            "train Loss: 0.0058 Acc: 1.0000\n",
            "val Loss: 0.5647 Acc: 0.8150\n",
            "\n",
            "Epoch 14/24\n",
            "----------\n",
            "train Loss: 0.0058 Acc: 1.0000\n",
            "val Loss: 0.5563 Acc: 0.8050\n",
            "\n",
            "Epoch 15/24\n",
            "----------\n",
            "train Loss: 0.0069 Acc: 1.0000\n",
            "val Loss: 0.5584 Acc: 0.8150\n",
            "\n",
            "Epoch 16/24\n",
            "----------\n",
            "train Loss: 0.0043 Acc: 1.0000\n",
            "val Loss: 0.5460 Acc: 0.8300\n",
            "\n",
            "Epoch 17/24\n",
            "----------\n",
            "train Loss: 0.0033 Acc: 1.0000\n",
            "val Loss: 0.5763 Acc: 0.8050\n",
            "\n",
            "Epoch 18/24\n",
            "----------\n",
            "train Loss: 0.0057 Acc: 1.0000\n",
            "val Loss: 0.5864 Acc: 0.8100\n",
            "\n",
            "Epoch 19/24\n",
            "----------\n",
            "train Loss: 0.0022 Acc: 1.0000\n",
            "val Loss: 0.5719 Acc: 0.8000\n",
            "\n",
            "Epoch 20/24\n",
            "----------\n",
            "train Loss: 0.0051 Acc: 1.0000\n",
            "val Loss: 0.5690 Acc: 0.8100\n",
            "\n",
            "Epoch 21/24\n",
            "----------\n",
            "train Loss: 0.0031 Acc: 1.0000\n",
            "val Loss: 0.5621 Acc: 0.8250\n",
            "\n",
            "Epoch 22/24\n",
            "----------\n",
            "train Loss: 0.0028 Acc: 1.0000\n",
            "val Loss: 0.5641 Acc: 0.8200\n",
            "\n",
            "Epoch 23/24\n",
            "----------\n",
            "train Loss: 0.0018 Acc: 1.0000\n",
            "val Loss: 0.5713 Acc: 0.8100\n",
            "\n",
            "Epoch 24/24\n",
            "----------\n",
            "train Loss: 0.0031 Acc: 1.0000\n",
            "val Loss: 0.5781 Acc: 0.8100\n",
            "\n",
            "Training complete in 3m 40s\n",
            "Best val Acc: 0.835000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### All Data Run with Adam"
      ],
      "metadata": {
        "id": "hcMKg98K4h9J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "resnet_model = models.resnet18(pretrained=True)\n",
        "in_features = resnet_model.fc.in_features\n",
        "resnet_model.classifier = nn.Linear(in_features, 2)\n",
        "resnet_model.to(device)\n",
        "\n",
        "loss_fn = torch.nn.CrossEntropyLoss()\n",
        "#optimizer = torch.optim.SGD(resnet_model.parameters(), lr=0.001, momentum=0.9)\n",
        "optimizer = torch.optim.AdamW(resnet_model.parameters(), lr=0.001)\n",
        "\n",
        "epochs = 25"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XGqL6ZUE4Ynk",
        "outputId": "c5a70d61-bc3a-44ce-a669-301c96c179b8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "resnet_model_all_adam, ressetnet_all_sgd_results = train_model(resnet_model, resnet_all_dict,\n",
        "                                                                loss_fn, optimizer, num_epochs = epochs)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kDzVbdKk4lni",
        "outputId": "f4389aa5-2516-452b-a9a3-ffe59044e141"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0/24\n",
            "----------\n",
            "train Loss: 2.4778 Acc: 0.6050\n",
            "val Loss: 4.5733 Acc: 0.4950\n",
            "\n",
            "Epoch 1/24\n",
            "----------\n",
            "train Loss: 0.4589 Acc: 0.8237\n",
            "val Loss: 0.4395 Acc: 0.8100\n",
            "\n",
            "Epoch 2/24\n",
            "----------\n",
            "train Loss: 0.2774 Acc: 0.8938\n",
            "val Loss: 0.4589 Acc: 0.8250\n",
            "\n",
            "Epoch 3/24\n",
            "----------\n",
            "train Loss: 0.1932 Acc: 0.9200\n",
            "val Loss: 0.4237 Acc: 0.7900\n",
            "\n",
            "Epoch 4/24\n",
            "----------\n",
            "train Loss: 0.1359 Acc: 0.9513\n",
            "val Loss: 0.5484 Acc: 0.8200\n",
            "\n",
            "Epoch 5/24\n",
            "----------\n",
            "train Loss: 0.1035 Acc: 0.9537\n",
            "val Loss: 1.6148 Acc: 0.6450\n",
            "\n",
            "Epoch 6/24\n",
            "----------\n",
            "train Loss: 0.1029 Acc: 0.9675\n",
            "val Loss: 1.3119 Acc: 0.7000\n",
            "\n",
            "Epoch 7/24\n",
            "----------\n",
            "train Loss: 0.0490 Acc: 0.9850\n",
            "val Loss: 1.0390 Acc: 0.7800\n",
            "\n",
            "Epoch 8/24\n",
            "----------\n",
            "train Loss: 0.0711 Acc: 0.9750\n",
            "val Loss: 0.7780 Acc: 0.7800\n",
            "\n",
            "Epoch 9/24\n",
            "----------\n",
            "train Loss: 0.1032 Acc: 0.9650\n",
            "val Loss: 0.8964 Acc: 0.7500\n",
            "\n",
            "Epoch 10/24\n",
            "----------\n",
            "train Loss: 0.0877 Acc: 0.9700\n",
            "val Loss: 0.6413 Acc: 0.8600\n",
            "\n",
            "Epoch 11/24\n",
            "----------\n",
            "train Loss: 0.0590 Acc: 0.9750\n",
            "val Loss: 0.5099 Acc: 0.8200\n",
            "\n",
            "Epoch 12/24\n",
            "----------\n",
            "train Loss: 0.0377 Acc: 0.9875\n",
            "val Loss: 0.7773 Acc: 0.8050\n",
            "\n",
            "Epoch 13/24\n",
            "----------\n",
            "train Loss: 0.0591 Acc: 0.9750\n",
            "val Loss: 0.9584 Acc: 0.7500\n",
            "\n",
            "Epoch 14/24\n",
            "----------\n",
            "train Loss: 0.0622 Acc: 0.9775\n",
            "val Loss: 1.0252 Acc: 0.8000\n",
            "\n",
            "Epoch 15/24\n",
            "----------\n",
            "train Loss: 0.0550 Acc: 0.9763\n",
            "val Loss: 1.1281 Acc: 0.7550\n",
            "\n",
            "Epoch 16/24\n",
            "----------\n",
            "train Loss: 0.0728 Acc: 0.9775\n",
            "val Loss: 1.1413 Acc: 0.7600\n",
            "\n",
            "Epoch 17/24\n",
            "----------\n",
            "train Loss: 0.0590 Acc: 0.9813\n",
            "val Loss: 1.5268 Acc: 0.6650\n",
            "\n",
            "Epoch 18/24\n",
            "----------\n",
            "train Loss: 0.0961 Acc: 0.9613\n",
            "val Loss: 2.3603 Acc: 0.6300\n",
            "\n",
            "Epoch 19/24\n",
            "----------\n",
            "train Loss: 0.0384 Acc: 0.9888\n",
            "val Loss: 1.2053 Acc: 0.7600\n",
            "\n",
            "Epoch 20/24\n",
            "----------\n",
            "train Loss: 0.0334 Acc: 0.9888\n",
            "val Loss: 0.8592 Acc: 0.8150\n",
            "\n",
            "Epoch 21/24\n",
            "----------\n",
            "train Loss: 0.0287 Acc: 0.9913\n",
            "val Loss: 0.8645 Acc: 0.7950\n",
            "\n",
            "Epoch 22/24\n",
            "----------\n",
            "train Loss: 0.0674 Acc: 0.9725\n",
            "val Loss: 1.2069 Acc: 0.7800\n",
            "\n",
            "Epoch 23/24\n",
            "----------\n",
            "train Loss: 0.0469 Acc: 0.9838\n",
            "val Loss: 0.6756 Acc: 0.8050\n",
            "\n",
            "Epoch 24/24\n",
            "----------\n",
            "train Loss: 0.1003 Acc: 0.9650\n",
            "val Loss: 0.9355 Acc: 0.7850\n",
            "\n",
            "Training complete in 4m 4s\n",
            "Best val Acc: 0.860000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Vision Transformer Model"
      ],
      "metadata": {
        "id": "OnTlaXy_SO5W"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://github.com/NielsRogge/Transformers-Tutorials/blob/master/VisionTransformer/Fine_tuning_the_Vision_Transformer_on_CIFAR_10_with_the_🤗_Trainer.ipynb"
      ],
      "metadata": {
        "id": "xCsU_kXES1xl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# dicts for our labels and targets\n",
        "target_to_label = {'real' : 0, 'fake': 1}\n",
        "label_to_target = {0 : 'real', 1 : 'fake'}"
      ],
      "metadata": {
        "id": "lbEu6rLOo_9A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# transform our dataset to match pre trained model\n",
        "def transform(example_batch):\n",
        "    # Take a list of PIL images and turn them to pixel value\n",
        "    images = []\n",
        "    for img_path in example_batch['image_path']:\n",
        "        images.append(io.imread(img_path))\n",
        "    inputs = feature_extractor([x for x in images], return_tensors='pt')\n",
        "\n",
        "    # Don't forget to include the labels!\n",
        "    targets = []\n",
        "    for targ in example_batch['target']:\n",
        "        targets.append(target_to_label[targ])\n",
        "    inputs['labels'] = [x for x in targets]\n",
        "    return inputs"
      ],
      "metadata": {
        "id": "LzkD0wI3b2Me"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# create hugging face datasets\n",
        "ViT_train_1000 = Dataset.from_pandas(train_1000).with_transform(transform)\n",
        "ViT_train_all = Dataset.from_pandas(train_all).with_transform(transform)\n",
        "ViT_test_1000 = Dataset.from_pandas(test_1000).with_transform(transform)\n",
        "ViT_test_all = Dataset.from_pandas(test_1000).with_transform(transform)"
      ],
      "metadata": {
        "id": "GXW8_iNsSxXu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://huggingface.co/google/vit-base-patch16-224-in21k"
      ],
      "metadata": {
        "id": "UTEUNP8v2t2E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# our pre-trained model we are using trained on ImageNet-21k\n",
        "model_name_or_path = 'google/vit-base-patch16-224-in21k'\n",
        "feature_extractor = ViTFeatureExtractor.from_pretrained(model_name_or_path)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0zxh9OwMWGM5",
        "outputId": "b7d0b4f7-b85f-491d-939c-58b1cc8bc50f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/transformers/models/vit/feature_extraction_vit.py:28: FutureWarning: The class ViTFeatureExtractor is deprecated and will be removed in version 5 of Transformers. Please use ViTImageProcessor instead.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# translate labels dicts\n",
        "target_to_label = {'real' : 0, 'fake': 1}\n",
        "label_to_target = {0 : 'real', 1 : 'fake'}"
      ],
      "metadata": {
        "id": "QEt9llOzhRCy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# define collate function for combime our batches\n",
        "def collate_fn(batch):\n",
        "    return {\n",
        "        'pixel_values': torch.stack([x['pixel_values'] for x in batch]),\n",
        "        'labels': torch.tensor([x['labels'] for x in batch])\n",
        "    }"
      ],
      "metadata": {
        "id": "HyEnYD8fltzX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# metric to evaluate our model\n",
        "metric = load_metric(\"accuracy\")\n",
        "def compute_metrics(p):\n",
        "    return metric.compute(predictions=np.argmax(p.predictions, axis=1), references=p.label_ids)"
      ],
      "metadata": {
        "id": "t98hn5XNoBZo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ViT Subset Run"
      ],
      "metadata": {
        "id": "aI6PdHvYxybz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# set our training args\n",
        "training_args = TrainingArguments(\n",
        "  output_dir=\"face/vit\",\n",
        "  per_device_train_batch_size=32,\n",
        "  evaluation_strategy=\"steps\",\n",
        "  num_train_epochs=5,\n",
        "  fp16=True,\n",
        "  save_steps=100,\n",
        "  eval_steps=100,\n",
        "  logging_steps=10,\n",
        "  learning_rate=2e-4,\n",
        "  save_total_limit=2,\n",
        "  remove_unused_columns=False,\n",
        "  push_to_hub=False,\n",
        "  report_to='tensorboard',\n",
        "  load_best_model_at_end=True,\n",
        ")"
      ],
      "metadata": {
        "id": "AqROc6MEo9vK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# create our model with subset data\n",
        "ViT_1000_model = ViTForImageClassification.from_pretrained(\n",
        "    model_name_or_path,\n",
        "    num_labels = 2,\n",
        "    id2label = label_to_target,\n",
        "    label2id = target_to_label\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B43lEKispCML",
        "outputId": "5332b648-1049-492e-f108-3c319628e851"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at google/vit-base-patch16-224-in21k were not used when initializing ViTForImageClassification: ['pooler.dense.bias', 'pooler.dense.weight']\n",
            "- This IS expected if you are initializing ViTForImageClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing ViTForImageClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of ViTForImageClassification were not initialized from the model checkpoint at google/vit-base-patch16-224-in21k and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# create our trainer to train and evalute model\n",
        "trainer_1000 = Trainer(\n",
        "    model = ViT_1000_model,\n",
        "    args = training_args,\n",
        "    data_collator = collate_fn,\n",
        "    compute_metrics = compute_metrics,\n",
        "    train_dataset = ViT_train_1000,\n",
        "    eval_dataset = ViT_test_1000,\n",
        "    tokenizer = feature_extractor,\n",
        ")"
      ],
      "metadata": {
        "id": "HzM5EZTJsuqt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# train our model and save results\n",
        "train_1000_results = trainer_1000.train()\n",
        "trainer_1000.save_model()\n",
        "trainer_1000.log_metrics(\"train\", train_1000_results.metrics)\n",
        "trainer_1000.save_metrics(\"train\", train_1000_results.metrics)\n",
        "trainer_1000.save_state()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 225
        },
        "id": "_6wojFA2tVUP",
        "outputId": "6afcd66b-bb8a-4266-d432-6d0270399ac9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='125' max='125' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [125/125 01:57, Epoch 5/5]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Accuracy</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>100</td>\n",
              "      <td>0.079500</td>\n",
              "      <td>0.470029</td>\n",
              "      <td>0.855000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "***** train metrics *****\n",
            "  epoch                    =         5.0\n",
            "  total_flos               = 288680157GF\n",
            "  train_loss               =      0.0502\n",
            "  train_runtime            =  0:01:58.93\n",
            "  train_samples_per_second =      33.632\n",
            "  train_steps_per_second   =       1.051\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(train_1000_results)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LewbFahzz0fi",
        "outputId": "0163cf76-2c2a-4c99-80eb-199ec64046f9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TrainOutput(global_step=125, training_loss=0.050203990936279294, metrics={'train_runtime': 118.9353, 'train_samples_per_second': 33.632, 'train_steps_per_second': 1.051, 'total_flos': 3.09967958458368e+17, 'train_loss': 0.050203990936279294, 'epoch': 5.0})\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# evaluate our results\n",
        "metrics_1000 = trainer_1000.evaluate(ViT_test_1000)\n",
        "trainer_1000.log_metrics(\"eval\", metrics_1000)\n",
        "trainer_1000.save_metrics(\"eval\", metrics_1000)"
      ],
      "metadata": {
        "id": "6W7K-sLavvVh",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 156
        },
        "outputId": "758767b9-4f19-4742-c1b2-25923fd6877e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='50' max='25' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [25/25 00:31]\n",
              "    </div>\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "***** eval metrics *****\n",
            "  epoch                   =        5.0\n",
            "  eval_accuracy           =      0.855\n",
            "  eval_loss               =       0.47\n",
            "  eval_runtime            = 0:00:02.43\n",
            "  eval_samples_per_second =     82.149\n",
            "  eval_steps_per_second   =     10.269\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ViT All Data Run"
      ],
      "metadata": {
        "id": "5oE0oGxBzUP8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# set our training args\n",
        "training_args = TrainingArguments(\n",
        "  output_dir=\"face/vit\",\n",
        "  per_device_train_batch_size=32,\n",
        "  evaluation_strategy=\"steps\",\n",
        "  num_train_epochs=25,\n",
        "  fp16=True,\n",
        "  save_steps=100,\n",
        "  eval_steps=100,\n",
        "  logging_steps=10,\n",
        "  learning_rate=2e-4,\n",
        "  save_total_limit=2,\n",
        "  remove_unused_columns=False,\n",
        "  push_to_hub=False,\n",
        "  report_to='tensorboard',\n",
        "  load_best_model_at_end=True,\n",
        ")"
      ],
      "metadata": {
        "id": "XmzHRtm70NEB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# create model with all of our data\n",
        "ViT_all_model = ViTForImageClassification.from_pretrained(\n",
        "    model_name_or_path,\n",
        "    num_labels = 2,\n",
        "    id2label = label_to_target,\n",
        "    label2id = target_to_label\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bDa0idDxzjbj",
        "outputId": "52490e13-bc78-4003-c534-46a82a7ec665"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at google/vit-base-patch16-224-in21k were not used when initializing ViTForImageClassification: ['pooler.dense.bias', 'pooler.dense.weight']\n",
            "- This IS expected if you are initializing ViTForImageClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing ViTForImageClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of ViTForImageClassification were not initialized from the model checkpoint at google/vit-base-patch16-224-in21k and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# create trainer to train and evaluate our model\n",
        "trainer_all = Trainer(\n",
        "    model = ViT_all_model,\n",
        "    args = training_args,\n",
        "    data_collator = collate_fn,\n",
        "    compute_metrics = compute_metrics,\n",
        "    train_dataset = ViT_train_all,\n",
        "    eval_dataset = ViT_test_all,\n",
        "    tokenizer = feature_extractor,\n",
        ")"
      ],
      "metadata": {
        "id": "j0vbjKZbzTxw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# train our model and save results\n",
        "train_all_results = trainer_all.train()\n",
        "trainer_all.save_model()\n",
        "trainer_all.log_metrics(\"train\", train_all_results.metrics)\n",
        "trainer_all.save_metrics(\"train\", train_all_results.metrics)\n",
        "trainer_all.save_state()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 434
        },
        "id": "2_wAT69Z0T1A",
        "outputId": "88fc0535-3276-4ec4-f7bb-87490b8fbf7b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/transformers/optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='625' max='625' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [625/625 09:08, Epoch 25/25]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Accuracy</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>100</td>\n",
              "      <td>0.257800</td>\n",
              "      <td>0.466254</td>\n",
              "      <td>0.830000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>200</td>\n",
              "      <td>0.066000</td>\n",
              "      <td>1.052370</td>\n",
              "      <td>0.750000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>300</td>\n",
              "      <td>0.003400</td>\n",
              "      <td>0.939797</td>\n",
              "      <td>0.790000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>400</td>\n",
              "      <td>0.002200</td>\n",
              "      <td>1.101762</td>\n",
              "      <td>0.780000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>500</td>\n",
              "      <td>0.001700</td>\n",
              "      <td>1.184804</td>\n",
              "      <td>0.775000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>600</td>\n",
              "      <td>0.001500</td>\n",
              "      <td>1.213172</td>\n",
              "      <td>0.775000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "***** train metrics *****\n",
            "  epoch                    =         25.0\n",
            "  total_flos               = 1443400785GF\n",
            "  train_loss               =       0.0944\n",
            "  train_runtime            =   0:09:10.56\n",
            "  train_samples_per_second =       36.327\n",
            "  train_steps_per_second   =        1.135\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# evaluate our model\n",
        "metrics_all = trainer_all.evaluate(ViT_test_all)\n",
        "trainer_all.log_metrics(\"eval\", metrics_all)\n",
        "trainer_all.save_metrics(\"eval\", metrics_all)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 156
        },
        "id": "5snfmTJp0a5q",
        "outputId": "1459927e-095d-4a1b-d884-2b69e87a5e08"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='25' max='25' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [25/25 00:01]\n",
              "    </div>\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "***** eval metrics *****\n",
            "  epoch                   =       25.0\n",
            "  eval_accuracy           =       0.83\n",
            "  eval_loss               =     0.4663\n",
            "  eval_runtime            = 0:00:02.11\n",
            "  eval_samples_per_second =     94.521\n",
            "  eval_steps_per_second   =     11.815\n"
          ]
        }
      ]
    }
  ]
}